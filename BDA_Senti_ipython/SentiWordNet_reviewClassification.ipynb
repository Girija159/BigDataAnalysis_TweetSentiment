{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import csv\n",
      "import nltk\n",
      "from nltk.corpus import wordnet\n",
      "import re\n",
      "import codecs\n",
      "import pprint\n",
      "\n",
      "class SentiWordNetCorpusReader:\n",
      "    def __init__(self, filename):\n",
      "        \"\"\"\n",
      "        Argument:\n",
      "        filename -- the name of the text file containing the\n",
      "                    SentiWordNet database\n",
      "        \"\"\"        \n",
      "        self.filename = filename\n",
      "        self.db = {}\n",
      "        self.parse_src_file()\n",
      "\n",
      "    def parse_src_file(self):\n",
      "        lines = codecs.open(self.filename, \"r\", \"utf8\").read().splitlines()\n",
      "        lines = filter((lambda x : not re.search(r\"^\\s*#\", x)), lines)\n",
      "        for i, line in enumerate(lines):\n",
      "            fields = re.split(r\"\\t+\", line)\n",
      "            fields = map(unicode.strip, fields)\n",
      "            try:            \n",
      "                pos, offset, pos_score, neg_score, synset_terms, gloss = fields\n",
      "            except:\n",
      "                sys.stderr.write(\"Line %s formatted incorrectly: %s\\n\" % (i, line))\n",
      "            if pos and offset:\n",
      "                offset = int(offset)\n",
      "                self.db[(pos, offset)] = (float(pos_score), float(neg_score))\n",
      "\n",
      "    def senti_synset(self, *vals):        \n",
      "        if tuple(vals) in self.db:\n",
      "            pos_score, neg_score = self.db[tuple(vals)]\n",
      "            pos, offset = vals\n",
      "            synset = wordnet._synset_from_pos_and_offset(pos, offset)\n",
      "            return SentiSynset(pos_score, neg_score, synset)\n",
      "        else:\n",
      "            synset = wordnet.synset(vals[0])\n",
      "            pos = synset.pos\n",
      "            offset = synset.offset\n",
      "            if (pos, offset) in self.db:\n",
      "                pos_score, neg_score = self.db[(pos, offset)]\n",
      "                return SentiSynset(pos_score, neg_score, synset)\n",
      "            else:\n",
      "                return None\n",
      "\n",
      "    def senti_synsets(self, string, pos=None):\n",
      "        sentis = []\n",
      "        synset_list = wordnet.synsets(string, pos)\n",
      "        for synset in synset_list:\n",
      "            sentis.append(self.senti_synset(synset.name))\n",
      "        sentis = filter(lambda x : x, sentis)\n",
      "        return sentis\n",
      "\n",
      "    def all_senti_synsets(self):\n",
      "        for key, fields in self.db.iteritems():\n",
      "            pos, offset = key\n",
      "            pos_score, neg_score = fields\n",
      "            synset = wordnet._synset_from_pos_and_offset(pos, offset)\n",
      "            yield SentiSynset(pos_score, neg_score, synset)\n",
      "\n",
      "class SentiSynset:\n",
      "    def __init__(self, pos_score, neg_score, synset):\n",
      "        self.pos_score = pos_score\n",
      "        self.neg_score = neg_score\n",
      "        self.obj_score = 1.0 - (self.pos_score + self.neg_score)\n",
      "        self.synset = synset\n",
      "\n",
      "    def __str__(self):\n",
      "        \"\"\"Prints just the Pos/Neg scores for now.\"\"\"\n",
      "        s = \"\"\n",
      "        s += self.synset.name + \"\\t\"\n",
      "        s += \"PosScore: %s\\t\" % self.pos_score \n",
      "        s += \"NegScore: %s\" % self.neg_score\n",
      "        return s\n",
      "\n",
      "    def __repr__(self):\n",
      "        return \"Senti\" + repr(self.synset)\n",
      "\n",
      "def tweet_dict(twitterData):  \n",
      "    ''' (file) -> list of dictionaries\n",
      "    This method should take your .csv\n",
      "    file and create a list of dictionaries.\n",
      "    '''\n",
      "    twitter_list_dict = []    \n",
      "    twitterfile = open(twitterData)\n",
      "    twitterreader = csv.reader(twitterfile)\n",
      "    for line in twitterreader:\n",
      "        twitter_list_dict.append(line[0])\n",
      "    return twitter_list_dict\n",
      "\n",
      "# return true if a string ia a stopword\n",
      "def is_stopword(string):\n",
      "    if string.lower() in nltk.corpus.stopwords.words('english'):\n",
      "        return True\n",
      "    else:\n",
      "        return False\n",
      "\n",
      "    # return true if a string is punctation    \n",
      "def is_punctuation(string):\n",
      "    for char in string:\n",
      "        if char.isalpha() or char.isdigit():\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "# Translation from nltk to Wordnet (words tag) (code)\n",
      "def wordnet_pos_code(tag):\n",
      "    if tag.startswith('NN'):\n",
      "        return wordnet.NOUN\n",
      "    elif tag.startswith('VB'):\n",
      "        return wordnet.VERB\n",
      "    elif tag.startswith('JJ'):\n",
      "        return wordnet.ADJ\n",
      "    elif tag.startswith('RB'):\n",
      "        return wordnet.ADV\n",
      "    else:\n",
      "        return ''\n",
      "\n",
      "    \n",
      "# Translation from nltk to Wordnet (words tag) (label)\n",
      "def wordnet_pos_label(tag):\n",
      "    if tag.startswith('NN'):\n",
      "        return \"Noun\"\n",
      "    elif tag.startswith('VB'):\n",
      "        return \"Verb\"\n",
      "    elif tag.startswith('JJ'):\n",
      "        return \"Adjective\"\n",
      "    elif tag.startswith('RB'):\n",
      "        return \"Adverb\"\n",
      "    else:\n",
      "        return tag\n",
      "    \n",
      "\n",
      "\"\"\" input -> a sentence \n",
      "    otput -> sentence in which each words is enriched of -> lemma, wordnet_pos, wordnet_definitions \n",
      "\n",
      "\"\"\"\n",
      "def wordnet_definitions(sentence):\n",
      "    wnl = nltk.WordNetLemmatizer()\n",
      "    for token in sentence:\n",
      "        word = token['word']\n",
      "        wn_pos = wordnet_pos_code(token['pos'])\n",
      "        if is_punctuation(word):\n",
      "            token['punct'] = True\n",
      "        elif is_stopword(word):\n",
      "            pass\n",
      "        elif len(wordnet.synsets(word, wn_pos)) > 0:\n",
      "            token['wn_lemma'] = wnl.lemmatize(word.lower())\n",
      "            token['wn_pos'] = wordnet_pos_label(token['pos'])\n",
      "            defs = [sense.definition for sense in wordnet.synsets(word, wn_pos)]\n",
      "            token['wn_def'] = \"; \\n\".join(defs) \n",
      "        else:\n",
      "            pass\n",
      "    return sentence\n",
      "\n",
      "\n",
      "#Tokenization\n",
      "\n",
      "def tag_tweet(tweet):    \n",
      "    sents = nltk.sent_tokenize(tweet)\n",
      "    sentence = []\n",
      "    for sent in sents:\n",
      "        tokens = nltk.word_tokenize(sent)\n",
      "        tag_tuples = nltk.pos_tag(tokens)\n",
      "        for (string, tag) in tag_tuples:\n",
      "            token = {'word':string, 'pos':tag}            \n",
      "            sentence.append(token)    \n",
      "    return sentence\n",
      "\n",
      "\n",
      "\n",
      "# WSD\n",
      "\n",
      "def word_sense_disambiguate(word, wn_pos, tweet):\n",
      "    senses = wordnet.synsets(word, wn_pos)\n",
      "    if len(senses) >0:\n",
      "        cfd = nltk.ConditionalFreqDist(\n",
      "               (sense, def_word)\n",
      "               for sense in senses\n",
      "               for def_word in sense.definition.split()\n",
      "               if def_word in tweet)\n",
      "        best_sense = senses[0] # start with first sense\n",
      "        for sense in senses:\n",
      "            try:\n",
      "                if cfd[sense].max() > cfd[best_sense].max():\n",
      "                    best_sense = sense\n",
      "            except: \n",
      "                pass                \n",
      "        return best_sense\n",
      "    else:\n",
      "        return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentiment = SentiWordNetCorpusReader(\"SentiWordNet_3.0.0_20130122.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#rev_bad_1.txt\n",
      "#rev_bad_2.txt\n",
      "#rev_good_b.txt\n",
      "#rev_good_s.txt\n",
      "#rev_good_t1.txt\n",
      "#rev_nutralbad_2.txt\n",
      "\n",
      "revfile = open(\"rev_nutralbad_2.txt\")\n",
      "\n",
      "review = revfile.read()\n",
      "\n",
      "review"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "\"I liked the look of this case at Best Buy versus the other they had available at the moment. I purchased this because I needed a case immediately and didn't want to risk carrying my new phone around with one. It seems like a study case and somewhat protective however it does not fit perfectly. It slides a little up and down in the case. The fit is fine left to right. I would probably look elsewhere for a case if I were you.\""
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "review = \"I bought this tablecloth in the taupe color for Thanksgiving dinner entertaining and was a little hesitant of what I would get for such a reasonable price. It washed well and didn't even need pressing after coming out of the dryer. The color worked out great with my gold-trimmed Lenox placesettings and the tablecloth was of a nice weight - not too flimsy yet not too heavy either. I'm pleased with this purchase and may order another in a smaller size for use now that the leaf is out of the table!\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = wordnet_definitions(tag_tweet(review))\n",
      "obj_score = 0 # object score \n",
      "pos_score=0 # positive score\n",
      "neg_score=0 #negative score\n",
      "pos_score_tre=0\n",
      "neg_score_tre=0\n",
      "threshold = 0.75\n",
      "count = 0\n",
      "count_tre = 0\n",
      "\n",
      "\"\"\"\n",
      "Conversion from plain text to SentiWordnet scores\n",
      "\"\"\"\n",
      " \n",
      "for word in a:\n",
      "    if 'punct' not in word :\n",
      "        sense = word_sense_disambiguate(word['word'], wordnet_pos_code(word['pos']), review)\n",
      "        \n",
      "        if sense is not None:\n",
      "            sent = sentiment.senti_synset(sense.name)\n",
      "            # Extraction of the scores\n",
      "            if sent is not None and sent.obj_score <> 1:\n",
      "                obj_score = obj_score + float(sent.obj_score)\n",
      "                pos_score = pos_score + float(sent.pos_score)\n",
      "                neg_score = neg_score + float(sent.neg_score)\n",
      "                count=count+1\n",
      "                print str(sent.pos_score)+ \" - \"+str(sent.neg_score)+ \" - \"+ str(sent.obj_score)+\" - \"+sent.synset.name\n",
      "                if sent.obj_score < threshold:\n",
      "                    pos_score_tre = pos_score_tre + float(sent.pos_score)\n",
      "                    neg_score_tre = neg_score_tre + float(sent.neg_score)\n",
      "                    count_tre=count_tre+1\n",
      "print review\n",
      "\n",
      "#Evaluation by different methods\n",
      "\n",
      "avg_pos_score=0\n",
      "avg_neg_score=0\n",
      "avg_neg_score_tre=0\n",
      "avg_neg_score_tre=0\n",
      "\n",
      "#2\n",
      "\n",
      "if count <> 0:\n",
      "    \n",
      "    avg_pos_score=pos_score/count\n",
      "    avg_neg_score=neg_score/count\n",
      "\n",
      "#3\n",
      "\n",
      "if count_tre <> 0:\n",
      "    avg_pos_score_tre=pos_score_tre/count_tre\n",
      "    avg_neg_score_tre=neg_score_tre/count_tre\n",
      "\n",
      "#pint results\n",
      "#1\n",
      "print \"pos_total : \"+str(pos_score)+\" - neg_ total: \"+str(neg_score)+\" - count : \"+str(count)+\" -> \"+(\" positivo \" if pos_score > neg_score else (\"negativo\" if pos_score < neg_score else \"neutro\"))\n",
      "#2\n",
      "print \"(AVG) pos : \"+str(avg_pos_score)+\" - (AVG) neg : \"+str(avg_neg_score)+\" -> \"+(\" positivo \" if avg_pos_score > avg_neg_score else (\"negativo\" if avg_pos_score < avg_neg_score else \"neutro\"))\n",
      "#3\n",
      "if count_tre > 0:\n",
      "    print \"(AVG_TRE) pos : \"+str(avg_pos_score_tre)+\" - (AVG_TRE) neg : \"+str(avg_neg_score_tre)+\" -> \"+(\" positivo \" if avg_pos_score_tre > avg_neg_score_tre else (\"negativo\" if avg_pos_score_tre < avg_neg_score_tre else \"neutro\"))\n",
      "print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.125 - 0.0 - 0.875 - like.v.05\n",
        "0.25 - 0.0 - 0.75 - best.n.01\n",
        "0.5 - 0.0 - 0.5 - bargain.n.02\n",
        "0.25 - 0.25 - 0.5 - necessitate.v.01\n",
        "0.0 - 0.375 - 0.625 - immediately.r.01\n",
        "0.25 - 0.0 - 0.75 - desire.v.01\n",
        "0.0 - 0.25 - 0.75 - risk.v.01\n",
        "0.25 - 0.0 - 0.75 - new.a.06\n",
        "0.125 - 0.125 - 0.75 - appear.v.04\n",
        "0.125 - 0.0 - 0.875 - study.n.02\n",
        "0.125 - 0.0 - 0.875 - protective.a.01\n",
        "0.125 - 0.5 - 0.375 - however.r.01\n",
        "0.0 - 0.625 - 0.375 - not.r.01\n",
        "0.375 - 0.125 - 0.5 - perfectly.r.02\n",
        "0.0 - 0.375 - 0.625 - little.r.01\n",
        "0.5 - 0.0 - 0.5 - fit.n.03\n",
        "0.25 - 0.125 - 0.625 - be.v.01\n",
        "0.0 - 0.125 - 0.875 - fine.n.01\n",
        "0.25 - 0.0 - 0.75 - right.r.01\n",
        "0.25 - 0.125 - 0.625 - be.v.01\n",
        "I liked the look of this case at Best Buy versus the other they had available at the moment. I purchased this because I needed a case immediately and didn't want to risk carrying my new phone around with one. It seems like a study case and somewhat protective however it does not fit perfectly. It slides a little up and down in the case. The fit is fine left to right. I would probably look elsewhere for a case if I were you.\n",
        "pos_total : 3.75 - neg_ total: 3.0 - count : 20 ->  positivo \n",
        "(AVG) pos : 0.1875 - (AVG) neg : 0.15 ->  positivo \n",
        "(AVG_TRE) pos : 0.225 - (AVG_TRE) neg : 0.25 -> negativo\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    }
   ],
   "metadata": {}
  }
 ]
}